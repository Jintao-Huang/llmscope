
# Gemma2ä¸­æ–‡å¢å¼ºæœ€ä½³å®è·µ

gemma2æ˜¯googleæå‡ºçš„æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹, åŒ…æ‹¬gemma2-9b, gemma2-9b-it, gemma2-27b, gemma2-27b-itå››ä¸ªæ¨¡å‹. è¯¥æ–‡æ¡£å°†å±•ç¤ºå¯¹gemma2-9b-instructä½¿ç”¨ç»è¿‡æ¸…æ´—çš„ä¸­è‹±æ–‡SFTé€šç”¨ã€ä»£ç å’Œæ•°å­¦æ•°æ®é›†è¿›è¡Œä¸­æ–‡å¢å¼ºï¼Œå¹¶ä½¿ç”¨è‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†ä¿®æ”¹æ¨¡å‹å¯¹è‡ªå·±å’Œä½œè€…çš„è®¤çŸ¥ã€‚

æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†é“¾æ¥å¦‚ä¸‹ï¼š
SFTæ•°æ®é›†ï¼šhttps://modelscope.cn/datasets/swift/swift-sft-mixture
è‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†ï¼šhttps://modelscope.cn/datasets/swift/self-cognition

## ç›®å½•
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [åŸå§‹æ¨¡å‹](#åŸå§‹æ¨¡å‹)
- [ä¸­æ–‡å¢å¼ºå¾®è°ƒ](#ä¸­æ–‡å¢å¼ºå¾®è°ƒ)
- [å¾®è°ƒåæ¨¡å‹](#å¾®è°ƒåæ¨¡å‹)

## ç¯å¢ƒå‡†å¤‡

```shell
# è®¾ç½®pipå…¨å±€é•œåƒ (åŠ é€Ÿä¸‹è½½)
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
# å®‰è£…ms-swift
git clone https://github.com/modelscope/swift.git
cd swift
pip install -e '.[llm]'
# å®‰è£…è¯„æµ‹ç›¸å…³ä¾èµ–
pip install -e '.[eval]'

# gemma2ä¾èµ–
pip install transformers>=4.42

# å¦‚æœè¦ä½¿ç”¨vllmå¯¹gemma2è¿›è¡Œæ¨ç†åŠ é€Ÿ, éœ€è¦ä½¿ç”¨æºä»£ç æ–¹å¼è¿›è¡Œå®‰è£… (or vllm>=0.5.1)
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

## åŸå§‹æ¨¡å‹

### æ¨ç†
```shell
# Experimental environment: A100
# å¦‚æœæ˜¯æœ¬åœ°æ¨¡å‹, éœ€æŒ‡å®š`--model_type gemma2-9b-instruct --model_id_or_path <local_path>`
# å¦‚æœä½¿ç”¨åŸç”Ÿpytorchè¿›è¡Œæ¨ç†, è¯·è®¾ç½®`--infer_backend pt`
CUDA_VISIBLE_DEVICES=0 swift infer \
    --model_id_or_path LLM-Research/gemma-2-9b-it \
    --infer_backend vllm
```

æˆ‘ä»¬ä½¿ç”¨å‡ ä¸ªæ ·ä¾‹è¿›è¡Œæµ‹è¯•ï¼Œå‘ç°å…¶ä¸­æ–‡èƒ½åŠ›å¹¶ä¸ç¨³å®šï¼š
```python
"""
<<< ä½ æ˜¯è°ï¼Ÿ
æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”± Google è®­ç»ƒã€‚

æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©äººä»¬é€šè¿‡ç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ¥å®Œæˆå„ç§ä»»åŠ¡ã€‚æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€æ€»ç»“æ–‡æœ¬ã€åˆ›ä½œæ•…äº‹ã€ç¿»è¯‘è¯­è¨€ç­‰ç­‰ã€‚

æˆ‘ä»ç„¶åœ¨å­¦ä¹ ä¸­ï¼Œä½†æˆ‘å¾ˆä¹æ„å°½åŠ›å¸®åŠ©ä½ ï¼
--------------------------------------------------
<<< what's your name?
I don't have a name. My creators, the Gemma team, haven't given me one. You can just call me Gemma!
--------------------------------------------------
<<< ä½ æ˜¯è°ç ”å‘çš„ï¼Ÿ
æˆ‘ç”± Google DeepMind ç ”å‘ã€‚ ğŸ˜Š 

è™½ç„¶æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†æˆ‘çš„å¼€å‘å’Œç»´æŠ¤æ˜¯ç”± Google DeepMind çš„å·¥ç¨‹å¸ˆå’Œç ”ç©¶äººå‘˜å®Œæˆçš„ã€‚
--------------------------------------------------
<<< æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ
æµ™æ±Ÿçš„çœä¼šæ˜¯ **æ­å·**ã€‚  ğŸ˜Š 



--------------------------------------------------
<<< è¿™æœ‰ä»€ä¹ˆå¥½åƒçš„ï¼Ÿ
æ­å·ç¾é£Ÿä¼—å¤šï¼ŒçœŸæ˜¯è®©äººå‚æ¶ä¸‰å°ºï¼ ğŸ¤¤

ä»¥ä¸‹æ˜¯ä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿï¼š

* **è¥¿æ¹–é†‹é±¼:** æ­å·çš„ä»£è¡¨èœï¼Œé±¼è‚‰é²œå«©ï¼Œé†‹æ±é…¸ç”œå¼€èƒƒï¼Œæ˜¯å¿…å°ä¹‹é€‰ã€‚
* **é¾™äº•èŒ¶:** æ­å·åèŒ¶ï¼Œæ¸…é¦™ç”˜é†‡ï¼Œæ˜¯å“èŒ¶çˆ±å¥½è€…çš„é¦–é€‰ã€‚
* **æ­å·å°ç¬¼åŒ…:** çš®è–„é¦…å¤šï¼Œæ±¤æ±é²œç¾ï¼Œæ˜¯æ­å·äººæ—©é¤çš„å¸¸å®¢ã€‚
* **è±†è…è„‘:**  æ­å·äººæ—©é¤çš„å¦ä¸€é€‰æ‹©ï¼Œé…ä¸Šé¦™æ²¹ã€è¾£æ¤’æ²¹å’Œè‘±èŠ±ï¼Œå‘³é“é²œç¾ã€‚
* **ç»å…´é…’:**  æµ™æ±Ÿç»å…´äº§çš„è‘—åé»„é…’ï¼Œé¦™é†‡ç”˜ç”œï¼Œæ˜¯å®´å¸­ä¸Šçš„ä½³é…¿ã€‚

è¿˜æœ‰å¾ˆå¤šå…¶ä»–ç¾å‘³ä½³è‚´ï¼Œæ¯”å¦‚ï¼š

* **ä¸œå¡è‚‰:**  ä»¥å…¶è‚¥è€Œä¸è…»ã€é¦™ç”œå¯å£çš„å£æ„Ÿè‘—ç§°ã€‚
* **å–µå‘œé±¼:**  ä»¥å…¶é²œå«©çš„é±¼è‚‰å’Œç‹¬ç‰¹çš„é…±æ±é—»åã€‚
* **æ¸…æ±¤é¢:**  æ­å·äººæ—¥å¸¸ç”Ÿæ´»ä¸­å¸¸è§çš„ç¾é£Ÿï¼Œæ±¤å¤´æ¸…æ¾ˆï¼Œé¢æ¡åŠ²é“ã€‚

å¦‚æœä½ æœ‰æœºä¼šå»æ­å·æ—…è¡Œï¼Œä¸€å®šè¦å“å°ä¸€ä¸‹è¿™äº›ç¾å‘³ä½³è‚´ï¼



--------------------------------------------------
"""
```

### è¯„æµ‹

```shell
# Experimental environment: A100
# è¯„æµ‹åç«¯ç”±llmusesåº“æä¾›: https://github.com/modelscope/eval-scope
# æ¨èä½¿ç”¨vllmè¿›è¡Œæ¨ç†åŠ é€Ÿ. å¦‚æœä½¿ç”¨åŸç”Ÿpytorchè¿›è¡Œæ¨ç†, è¯·è®¾ç½®`--infer_backend pt`
CUDA_VISIBLE_DEVICES=0 swift eval \
    --model_id_or_path LLM-Research/gemma-2-9b-it \
    --eval_dataset arc ceval gsm8k mmlu --eval_backend Native \
    --infer_backend vllm
```


| Model              | arc    | ceval  | gsm8k  | mmlu   |
| ------------------ | ------ | ------ | ------ | ------ |
| llama3-8b-instruct | 0.7628 | 0.5111 | 0.7475 | 0.6369 |
| gemma2-9b-instruct | 0.8797 | 0.5275 | 0.8143 | 0.627  |



## ä¸­æ–‡å¢å¼ºå¾®è°ƒ
è¿™é‡Œä¸ºäº†é™ä½è®­ç»ƒçš„æ—¶é—´ï¼Œå¯¹æ•°æ®é›†è¿›è¡Œäº†è¾ƒå°‘çš„é‡‡æ ·ã€‚å¦‚æœè¦æƒ³è¿›è¡Œæ›´å¤šæ•°æ®é›†çš„å¾®è°ƒï¼Œå¯ä»¥é€‚å½“å¢å¤§æ··åˆçš„æ¯”ä¾‹ï¼Œä¾‹å¦‚ï¼š`--dataset swift-mix:sharegpt#50000 swift-mix:firefly#20000 swift-mix:codefuse#20000 swift-mix:metamathqa#20000 self-cognition#1000`ã€‚

æˆ‘ä»¬å¯¹embeddingå±‚ã€æ‰€æœ‰çš„linearå±‚å’Œlm_headå±‚åŠ ä¸Šloraï¼Œå¹¶è®¾ç½®layer_normå±‚å¯è®­ç»ƒã€‚æˆ‘ä»¬åœ¨ä¸­æ–‡å¢å¼ºçš„åŒæ—¶ï¼Œä¿®æ”¹æ¨¡å‹çš„è‡ªæˆ‘è®¤çŸ¥ï¼Œè®©æ¨¡å‹è®¤ä¸ºè‡ªå·±æ˜¯å°é»„ï¼Œç”±é­”æ­åˆ›å»ºã€‚

```shell
# Experimental environment: 4 * A100
# 4 * 80GB GPU memory
# # å¦‚æœæ˜¯æœ¬åœ°æ¨¡å‹, éœ€æŒ‡å®š`--model_type gemma2-9b-instruct --model_id_or_path <local_path>`
MASTER_PORT=29500 \
NPROC_PER_NODE=4 \
CUDA_VISIBLE_DEVICES=0,1,2,3 \
swift sft  \
    --model_id_or_path LLM-Research/gemma-2-9b-it \
    --dataset swift-mix:sharegpt#10000 swift-mix:firefly#5000 swift-mix#5000 swift-mix:metamathqa#5000 self-cognition#500 \
    --lora_target_modules EMBEDDING ALL lm_head \
    --lora_modules_to_save LN \
    --adam_beta2 0.95 \
    --learning_rate 5e-5 \
    --num_train_epochs 5 \
    --eval_steps 100 \
    --max_length 8192 \
    --gradient_accumulation_steps 16 \
    --model_name å°é»„ 'Xiao Huang' \
    --model_author é­”æ­ ModelScope \
    --save_total_limit -1 \
    --logging_steps 5 \
    --use_flash_attn true \
```


## å¾®è°ƒåæ¨¡å‹

### æ¨ç†
```shell
# Experimental environment: A100
CUDA_VISIBLE_DEVICES=0 swift infer \
    --ckpt_dir output/gemma2-9b-instruct/vx-xxx/checkpoint-xxx \
    --infer_backend vllm --merge_lora true
```


### è¯„æµ‹
```shell
# Experimental environment: A100
CUDA_VISIBLE_DEVICES=0 swift eval \
    --ckpt_dir output/gemma2-9b-instruct/vx-xxx/checkpoint-xxx \
    --eval_dataset arc ceval gsm8k mmlu --eval_backend Native \
    --infer_backend vllm --merge_lora true
```

